\chapter{Theory}
\label{cha:theory}

In this section the theory behind the techniques used during the thesis work will be presented.
The first part will go through techniques used to process the data and perform an exploratory analysis.
After that text classifications, primarily with SVM, will be covered.
The last section contains an overview of the field of active learning, as well as a comparison of some different active learning techniques for multi-labeled data.

\section{Text Processing using Unsupervised Techniques}

Techniques in machine learning that does not require you have a categorized or labeled set of data is called unsupervised.
They use the structure of the data to obtain the information to use when processing it.
When it comes to text data, there are a few common methods and techniques that are unsupervised, and can be used for different purposes.
Examples of such techniques are \textit{topic modeling} and \textit{clustering}.
Another interesting technique is word2vec, that is used to produce word embeddings.

When working with text, it needs to be represented in a way that allows the models to work with it effectively.
\textit{Bag-of-words} (BoW) is one of the more common representations when performing text analysis. 
Using BoW the text is represented as a multi-set. 
That is, a document is represented by the number of occurrences of the different words. 
The representation of a document therefore becomes very high-dimensional, there is one dimension for each word in the vocabulary. 
Like the name implies, the positions of the words are not taken into account, they are viewed as if they were taken from a bag. 
Another drawback from this approach is that a word in a written language can be used to express several different thoughts, and one thought can be expressed using several different words. 
However, it is easy to work with, and is used when performing topic modeling among other things.

One way to incorporate positional information into the representation is the use of n-grams.
Instead of storing information pertaining to one term, information is stored with regards to n consecutive terms.
Considering the text ``Pattern Recognition and Machine Learning'' using a bigram (n-gram with n=2), would result in the tokens: ``Pattern Recognition'', ``Recognition and'', ``and Machine'', and ``Machine Learning''.

\subsection{Topic Modeling}

A topic model is a statistical model for finding topics within text~\cite{crain2012dimensionality}.
The topics build upon the probability that a certain word would occur in a text about a given topic, on the basis of terms occurring together.
For example, if the topic represents United States politics, words such as ``government'', ``Trump'', ``Reagan'', ``Senate'', or ``Medicaid'' are more likely to appear than ``sailboat'' or ``sweater''.
Any given document can then contain a topic with some probability.
This can be viewed as fuzzy clustering, and that the document has a degree of membership in a topic or cluster~\cite{crain2012dimensionality}.
The most common topic model in use is Latent Dirichlet Allocation (LDA)~\cite{crain2012dimensionality}.
Another topic model that preceded LDA is Probabilistic latent semantic analysis (PLSA)~\cite{hofmann1999probabilistic}.
However, PLSA has been shown to be more prone to overfitting than LDA~\cite{crain2012dimensionality}.

In the rest of the report, the following notation will be used:

\begin{itemize}
    \item D denotes a corpus of M documents: $D = \{w_1, w_2, \ldots, w_M\}$.
    \item The number of topics is $K$. Each topic is indexed by $i$.
    \item $N_d$ is the number of terms in document $d$.
    \item $N_i$ is the number of terms n topic $i$.
    \item $V$ denotes the number of words in the vocabulary.
\end{itemize}

\subsubsection{Latent Dirichlet Allocation}

Latent Dirichlet allocation (LDA) is a statistical model, where abstract topics in the model are defined as distributions over words~\cite{blei2003latent}.
LDA is based on a generative process, a model of which can be seen in Figure~\ref{fig:lda_gen_process}.
The circles in this figure represent random variables.
Dependencies between these random variables are shown with arrows, and if a variable is observed it is shaded in the figure.
In this model, the only observed variable is the words in the document.
Parts of the model are surrounded by a rectangle to show that the part is repeated several times.

\begin{figure}[!ht]
\includegraphics[scale=0.7]{figures/lda-generative-process.eps}
\caption{Diagram of the LDA model.}\label{fig:lda_gen_process}
\end{figure}

The generation of a corpus is done with the following steps~\cite{crain2012dimensionality, blei2003latent}:

\begin{itemize}
    \item \textbf{Draw a distribution over the words for each topic.}
        A sample $\phi_i$ is drawn from a symmetric Dirichlet distribution with parameter $\beta$. 
        This sample represents the distribution of terms for the topic $i$.

        \begin{equation}
            \Phi_i \sim Dir(\beta)
        \end{equation}

        \begin{equation}
            p(\Phi_i | \beta) = \frac{\Gamma(V\beta)}{{\Gamma(\beta)}^V} \prod^V_{v=1}\phi^{\beta-1}_{iv}
        \end{equation}

        Here, $\Gamma$ is the gamma function.

    \item \textbf{Draw a distribution over the topics for each document.}
        A sample $\theta_d$ is drawn from a Dirichlet distribution with parameters $\alpha$.
        This sample represents the distribution of topics for document $d$.

        \begin{equation}
            \Theta_d \sim Dir(\alpha)
        \end{equation}

        \begin{equation}
            p(\Theta_d | \alpha) = \frac{\Gamma(\sum^K_{i=1}\alpha_i)}{\prod_{i=1}^K\Gamma(\alpha_i)}\prod_{i=1}^K\theta_{di}^{\alpha_i-1}
        \end{equation}

    \item For each token with index n:

        \begin{itemize}
            \item \textbf{Draw a topic assignment $z_{dn}$ for the token index $n$.} 
                $z_{dn}$ is drawn from the distribution over topics for each document. 
                That is, $z_{dn}$ is drawn from a multinomial distribution using $\theta_d$ as a parameter.

                \begin{equation}
                    z_{dn} \sim Multinomial(\Theta_d)
                \end{equation}

                \begin{equation}
                    p(z_{dn} = i | \Theta_d) = \theta_{di}
                \end{equation}

            \item \textbf{Draw a token $w_{dn}$.}
                The token $w_{dn}$ is drawn from the topic distribution assigned to the index $n$.
                That is, $w_{dn}$ is drawn from a multinomial with parameter $\phi_{z_{dn}}$.

                \begin{equation}
                    w_{dn} \sim Multinomial(\Phi_{z_{dn}})
                \end{equation}

                \begin{equation}
                    p(w_{dn}=v|z_{dn}=i,\Phi_i) = \phi_{iv}
                \end{equation}

        \end{itemize}

\end{itemize}

The LDA model identifies topics from different terms that occur together.
Consider the case where an LDA model has been used to learn a number of topics.
Two terms that frequently occur together are then likely to be in the same topic.
So, if the same word has been used to express different thoughts, and the word has the same probability in two topics, the words that it co-occurs with can be used to differentiate between the different thoughts.

The task of learning the LDA model is a Bayesian Inference problem.
We have several variables that we cannot observe: the word distribution for the topics ($\phi_i$), the topic assignments for the tokens ($z$), and the topic distribution for the documents ($\theta_d$).
The only observed variables are the words in the document.
We have to approximate the posterior distribution using some sampling method, since it cannot be inferred automatically~\cite{blei2003latent}.

There exist a few algorithms that can be used to learn topics for the LDA model. 
Two of these that has shown to be able to extract useful topics from text are \textit{collapsed Gibbs sampling}~\cite{griffiths2004finding} and \textit{variational Bayes}~\cite{blei2003latent}.  
In collapsed Gibbs sampling, $\theta$ and $\phi$ are marginalized out.
It works by repeatedly sampling the topic assignment $z_{dn}$ for each token, conditioned on the assignments for the other tokens.
Variational Bayes works by using simpler single-variable models to approximate the LDA\@. 
As a consequence, it disregards any dependencies between the variables.
This is the approach used in the original LDA paper~\cite{blei2003latent}.

\subsubsection{Collapsed Gibbs}

% TODO: Write section on collapsed Gibbs
\textit{To be written}

\subsection{Text Clustering}

Cluster analysis is commonly defined as finding groups in a given dataset.
The members of these groups are determined to be similar by a similarity measure~\cite{kaufman2009finding, aggarwal2012survey}.
Since text data is sparse, but yet have a very high dimensionality.
With one dimension per term in the dictionary, it is not uncommon with dimensions in the order of $10^5$.
For this reason, some of the more naive clustering algorithms does not work as well for text data~\cite{aggarwal2012survey}.

In distance-based clustering, a similarity function is used to measure the closeness between two text documents.
For the purpose of measuring the similarity between text objects, the cosine similarity function is most commonly used~\cite{aggarwal2012survey}.
Two different approaches to distance-based clustering are distance-based partitioning, and agglomerative hierarchical clustering.
%For distance-based clustering k-means and k-medoid are two the frequently used algorithms.
%When it comes to text data, k-means is preferred since k-medoid does not work as well for sparse data, and requires more iterations to converge~\cite{aggarwal2012survey}.

\subsubsection{K-means Clustering}

When using the k-means clustering algorithm, the clusters are based upon an initial set of k representatives.
A simple approach to k-means clustering can be seen as:

\begin{enumerate}
    \item Select K seeds from the original dataset
    \item \label{enum:k-means-step-2} Assign the rest of the documents to one of these seeds, based how how similar they are by the similarity function
    \item \label{enum:k-means-step-3} Before each new iteration, select a new centroid for each cluster. This should be the point that is the best central point for the cluster.
    \item Repeat step \ref{enum:k-means-step-2} and \ref{enum:k-means-step-3} until convergence.
\end{enumerate}

A visualization of this can be seen in Figure~\ref{fig:kmeans-iterations}.
One advantage that K-means has over K-medoid is that it requires a small number of iterations, especially compared to K-medoid~\cite{aggarwal2012survey, schutze1997projections}.
However, K-means is rather sensitive to the selection of initial seeds.
One approach is to just select them randomly, or selecting them based on the result of another lightweight clustering method.
A frequently used method is k-means++, that has been shown to improve both the speed and accuracy of k-means clustering~\cite{arthur2007k}.

\begin{figure}[h!]
    \centering
    \thirdsubfig{kmeans-init}{Initial seeds}
    \thirdsubfig{kmeans-iter1}{Iteration 1}
    \newline
    \thirdsubfig{kmeans-init2}{Centroids after iteration 1}
    \thirdsubfig{kmeans-iter2}{Iteration 2}
    \thirdsubfig{kmeans-init3}{Centroids after iteration 2}
    \caption{(a) to (e) shows iterations of K-means until convergence.
        In (e) it can be seen that the new centroids capture the same documents as the previous iteration, and we have converged.}
    \label{fig:kmeans-iterations}
\end{figure}

\subsubsection{Hierarchical Clustering}

% TODO: Write if needed
\textit{To be written if used in the end}

\subsection{Word Embeddings with Word2Vec}

\textit{To be written}

\section{Text Classification}

Text classification is a widely studied field within Computer Science.
It is an important problem in supervised machine learning, and it is the task of assigning one or more classes to a given text document~\cite{aggarwal2012surveyclass}.
The problem is mainly approached with supervised machine learning.
That is, with a dataset that consists of a collection of text documents, where each document has one or more classes assigned to it. % TODO: QUOTE BISHOP
With the help of these labels, a classification model is fitted to the data.
The goal of this is for the model to be able to correctly assign a class to a a previously unseen text document.
Some of these classification models can also produce a probability of a document being of a certain class.
Other models are based on the concept of a margin that separates the classes, and the distance between a data point and a margin can be used to indicate how certain the model is of the assigned label~\cite{tong2001support}.
Example of use cases for text classification is categorization of news articles, document retrieval and email filtering.
There exists several different models for classifying text.
Decision trees, neural networks and Support Vector Machines (SVM) are some have been previously applied to the text domain with successful results~\cite{aggarwal2012survey}.
In this thesis, SVM are the main focus, since they have been studied extensively in the context of active learning. %TODO: CITE ALL OF THE SUPPORT VECTOR MACHINE ACTIVE LEARNING PAPERS

\subsection{Support Vector Machines}

SVMs work by implicitly map the training data to a feature space~\cite{bishop2006pattern}.
The goal is that the data should be linearly separable in the feature space, even if it is not in the input space.
In the case of binary classification, a point is classified by the linear model:

\begin{equation}
    y(x) = w^T \phi(x) + b
\end{equation}

The sign of $y(x)$ determines the label assigned to x.

SVMs work by trying to find the hyperplane that maximizes the margin.
That is, the distance between any point and the decision boundary should be as large as possible.
The hyperplane that gives us the maximum margin can be found by:

\begin{equation}
    \arg\min_{w,b}\frac{1}{2}||w||^2
\end{equation}

In order to allow for better generalization, and for data that isn't completely linearly separable, SVMs make use of slack variables $\xi_n$ to penalize points that are close to the decision boundary~\cite{bishop2006pattern}.
A parameter $C>0$ controls how much effect the slack variables will have.
The equation with the slack variables becomes:

\begin{equation}
    \arg\min_{w,b}\frac{1}{2}||w||^2 + C \sum_n\xi_n.
\end{equation}

A smaller $C$-value allows more points to be misclassified, in order to achieve better generalization.

\subsection{Multi-Label Classification}\label{subsec:multi-label-classification}

Multi-label classification is the type of text classification where one instance can be associated with multiple labels.
It is a generalized version of the multi-class classification problem, where you have more than 2 labels, but each document is only assigned one~\cite{tsoumakas2006multi}.

A common way of solving multi-label classification problems is the Binary Relevance method~\cite{read2011classifier, boutell2004learning}.
It is a way of transforming the multi-label classification problem into several different binary ones.
With Binary Relevance you fit one classification model per label in your data.
Each of these classifiers are then predicting whether or not the document is associated with the corresponding label or not.

\section{Active Learning}

Conventional machine learning systems use a set of available data to find a hypothesis that can explain the patterns.
The purpose of active learning is to allow a system to \textit{select} the data that it wants labeled, and therefore the data it wants to be trained on~\cite{settles2012active}.
An active learning system samples a document to be labeled, and then queries an oracle (often a human annotator) to get the label for that document.
By being able to decide what data to to label and use, the goal is that the system can achieve better results, and that the data will be of higher quality.
A model of the active learning system can be seen in Figure~\ref{fig:active-learning-model}.
The main 

\begin{figure}[!ht]
    \centering
    \includegraphics[scale=0.7]{figures/active-learning-model.eps}
    \caption{An overview of an active learning system}
    \label{fig:active-learning-model}
\end{figure}

In several different domains, data is readily available and easy to come by.
But even if the data is abundant, labels for the data is often harder and more expensive to come by~\cite{settles2012active}, especially when it comes to multi-label problems.

The next section will describe different ways to access the documents in active learning systems, followed by some theory of how the samples relate to the hypothesis space.
After that some concrete methods for selecting the samples to queried are described and compared.

\subsection{Pool-Based Sampling}

The main focus is how to select the samples to be labeled.
There are different sampling methods in use, and which one is more appropriate depends on how the data can be accessed.
Pool-based sampling is motivated by the assumption that there exists a large ready pool of data, where only a small portion is labeled~\cite{lewis1994sequential, settles2012active}.
The samples to be labeled is then selecting by evaluating the entire pool of unlabeled data, and select the most appropriate one based on a defined utility measure.
If the entire pool is large, a subset could be used instead.
For applied active learning, pool-based sampling seems to be the most popular choice, but there are some alternatives that have been used in theoretical settings such as stream-based selective sampling.
The difference between stream-based selective sampling and pool-based sampling is the individuality of the decisions in stream-based selective sampling, where you draw one sample at a time from an input source and make the decision whether or not to query a label for it.
For text applications, where a set of data is often readily available, pool-based sampling is often the more appropriate option since you can consider the entire dataset.
Pool-based is therefor the sampling technique that will be considered in this paper.

\subsection{Initial Sampling Selection}

\textit{To be written}

\subsection{Searching the Hypothesis Space}

In machine learning hypothesis is a specific configuration of a model, the purpose of which is to predict outputs on new instances of data by generalizing the training data.
One hypothesis can, for example, be an SVM model with specific values for the parameters.
The set of all possible hypothesis that we are working with the \textit{hypothesis space}.
Following the SVM example, the hypothesis space would be the set of SVM's with the different values that are possible for the parameters.
The hypothesis space is defined as:
% TODO: Big brackets
\begin{equation}
    \begin{aligned}
        \mathcal{H} = \Bigg \{ f | f(x) = \frac{w * \phi(x)}{|w|} \Bigg \}\\
        \text{where~$w \in \mathcal{W}$ and $\mathcal{W}$ is our parameter space}
    \end{aligned}
\end{equation}

The version space the subset of the hypothesis space
The subset of the hypothesis space that in the feature space separates the data is called the version space, which is defined as:

\begin{equation}
    \mathcal{V} = \Bigg \{ f \in \mathcal{H} | y_i f(x_i) > 0 \forall i \in \{i \dots n\} \Bigg \}
\end{equation}

So the version space therefore represents the different hypothesis that make correct predictions on the training data.
Under the assumption that one of the hypothesis can fully separate the data, the version space shrinks when more labeled data is acquired.
So for new labeled instances the hypotheses in the version space will give better predictions for the training data.
Based on this, an active learning algorithm should aim to reduce the size of the version space with each new sample, optimally make it half the size in each iteration.

There exists a useful relationship between the feature space $\mathcal{F}$ and the hypothesis space $\mathcal{H}$ called the \textit{version space duality}~\cite{tong2001support, vapnik1998statistical}.
It states that hyperplanes in the hypothesis corresponds to points in the feature space, and the other way around.
So by selecting points to be labeled, constraints can be enforced on the hypothesis that form the version space.

One approach to this is called that has shown to be successful is \textit{Uncertainty Sampling}~\cite{settles2012active}.
The idea behind SVM is to find a hyperplane that separates two classes in a binary classification with the maximum margin.
Out of the different hyperplanes in the hypothesis space, the version space contains those that can successfully separate the data.
We want to select the points in the feature space that will reduce the amount of the valid hypotheses the most.
Since SVMs tries to find the support vectors that maximizes the decision boundary in the feature space, separating the two classes.
Considering this in $\mathcal{H}$, it will be analogous to the hypothesis in the center of the hypothesis space encompassed constraints set by the labeled points.
What Uncertainty Sampling is predicting the values for the unlabeled points, and then choose the one that it is most uncertain about, the one closest to the decision boundary, to be labeled.
Based on the version space duality, it is a good approximation for dividing the version space in two.

\subsection{Binary Minimization}

Binary Minimization is a generalization of uncertainty sampling, to make it work with multi-label data.
The approach taken is to decompose the multi-label problem to several binary one-vs-rest tasks, like discussed in~\ref{subsec:multi-label-classification}.
The unlabeled point that is chosen for labeling is then the one with the smallest SVM margin across all the binary classification tasks.
By doing this, it does not incorporate the multiple labels into the decision process, but treats all classes individually and equally. 

\subsection{Maximum Loss Reduction with Maximum Confidence}

\subsection{Adaptive Active Learning}

\section{Related Work}

Active learning has been researched in text classification with different approaches.
They can be seen as two categories: searching through the hypothesis space by using the uncertainty of a model, or by exploiting the structure of the data through clustering~\cite{dasgupta2008hierarchical}.

One of the common baselines for active learning is uncertainty sampling~\cite{lewis1994sequential}.
That simply queries the label for the data point the model is most uncertain about.
In~\cite{dasgupta2008hierarchical} hierarchical clustering is used in an active learning system.
The labels are queried from clusters where there is a lot of uncertainty when it comes to the majority label.
By pruning the tree of clusters while querying for labels the goal is to obtain a pruning where each node mostly contains one label.

In \cite{nguyen2004active} they also take advantage of a clustering to select the samples to be labeled in a two-class environment.
They take advantage of that the data points closest to the centroids are the most important ones, and that most data points in one cluster have the same label.
What this approach has in common with a lot of the current research is that it is treating single-label or binary classification problems.