\chapter{Discussion}
\label{cha:discussion}

The discussion chapter is separated into three parts.
First, it goes through and analyzes the results from the experiments in Chapter~\ref{cha:experiments}.
This is followed by an analysis of the methods used to conduct those experiments.
The related works 
At last the work is discussed in a wider context, based on the ethical and societal impact techniques like these may have.

\section{Results}
\label{sec:discussion-results}

Here the results are analyzed and discussed for the different experiments in Chapter~\ref{cha:experiments}.
They are discussed based on their relation to the research questions.

\subsection{Experiment 1}

The first experiment was conducted to be able to answer the first research question, which treated how well the SVM model perform on the data labeled by the active learning process.
Looking at how the different strategies perform in terms of accuracy and $F_1$-score, it is clear that for the first 600-700 labels, Maximum Loss Reduction with Maximum Confidence (MMC) performs considerably worse than the rest.
That is including random sampling.
It is equally clear that it performs better when the initial sample is bigger.
One reason behind this may be MMC's dependence on the label cardinality of the samples.
MMC's dependence on label cardinality can be seen in Section~\ref{subsec:mmc}.
If the information on label cardinality is not varied, the predictions that MMC base its calculations upon will not be as good.
In the paper by Yang et al\@.~\cite{yang2009effective}, MMC performs significantly better than than Binary Version Space Minimization (BSVM) at all stages.
One obvious reason for this discrepancy could be the implementation used in this paper, as is discussed in Section~\ref{sec:discussion-results}.
Another one can be the initial sample sizes used in their paper, even when they compare different initial sample sizes, they start at 100 samples and go up to a 1000.

Adaptive Active Learning (AAL) performs the best of the evaluated strategies in both accuracy and $F_1$-score, when only a few hundred samples has been labeled. 
A probable reason for this is the extra computations the strategy does in order to find the best weight between the certainty based, and the cardinality based, strategies.
If the cardinality predictions is not deemed to be very good in the beginning, it would simply put more weight on the certainty measure, and vice versa.

BSVM on the other hand performs very similarly to random sampling for the first samples.
The reason for this is rather simple.
The strategy finds the class that is the closest to the decision boundary for each data point, and selects the minimum distance of those.
If there are several documents with the same minimum value they are selected at random from those.
Until all classes has been sampled at least one, the minimum for all data points will be the same class, and it will be equal among all data points.
Therefore, it will be the same as selecting randomly among all the samples.
However, when all classes have been sampled, and quickly outperforms the random sampling in terms of accuracy and $F_1$-score.

In the end, the different strategies all perform better than random sampling.
The ones that are based on MMC and BSVM tend to perform a bit better than AAL when more than 1200-1300 samples are added.
This may very well be related to the fact that the distributions for MMC and BSVM is considerably more uniform when a lot of labels have been requested.
BSVM is the only technique that reached 89 \% accuracy within the first 2500 labels to be labeled.
It is possible that other techniques would have reached it given more labels, but it is clear that BSVM reaches a peek accuracy earlier.
The technique always tries to sample from the most uncertain category.
The lack of reports in this category can be the thing that prohibits the other strategies from achieving a higher accuracy.

The initial clustering of reports did not seem to make as much of a difference as the strategy used.
This makes sense, the initial sample size only makes up for a little bit of the overall set of labeled reports.
Worth noting is that the SVM model performs worse for the first couple of iterations with initial clustering.
It quickly recovers after this and seems to perform as well, or slightly better, compared to the randomly initialized counterpart.
The reason for the low initial performance might be due to skewed dataset.
If we assume that the clusters captures a bit of the label information, then the labels should be more evenly distributed in the initial sample.
On the other hand, if the test set contains mostly one or two labels, the initial set will not have seen as much of these and will therefore not be as accurate in predicting them.
This would then greatly reduce the evaluation.

All clusters will not have the same number of members.
If most of the documents with a high label cardinality got assigned the same populous cluster, they might get neglected from the initial sample since an equal number of reports was taken from each cluster.
This could be a reason for the clustering approach to initial sampling only obtained samples with label cardinality 1, and therefore excluding MMC from the process.

\subsection{Experiment 2}

In order to satisfy the request of a more labeled dataset, which is of concern in the second research question, the distribution of labels were analyzed.
Looking at this distribution it is instantly clear that the three strategies outperforms random sampling significantly.
MMC and AAL are by far the most balanced one after only 500 labels are labeled.
When 2000 data points are labeled, BSVM is the most balanced one, albeit slightly compared to MMC.
It can probably be attributed to it selecting the most uncertain class in every iteration.
The most uncertain class is probably the one with the fewest samples, causing the distribution to be more uniform over time.

While BSVM always selects the sample that is the most uncertain in one single class, one can view MMC and AAL as more of an average of the uncertainty over the categories for a data point.
They also incorporate the label cardinality information.
This averaging trait may be beneficial for the distribution early on.
The model is most likely rather uncertain about most categories, so instead of focusing on the one with the absolutely lowest value they select the data point that will give the overall most information.
As with the evaluation on model performance, BSVM seem to perform better in the long run.
One could include MMC in this too, the difference between the two could probably be neglected in most applications.
However, if only a few hundred labels are supposed to be labeled, AAL is unrivaled when all metrics are considered.

\subsection{Experiment 3}

The last experiment is conducted in order to answer the third research question, related to how well the unsupervised techniques can be used to filter out invalid reports.
Obtaining 97.9 \% and 99.1 \% accuracy for classifying invalid reports must be considered a good result.
The reports describing the cases where an examination did not happen uses a fairly different vocabulary.
For these cases there is a lack of medical terms, while expressions such as ``canceled'', or ``new time'' are a lot more common.
This distinction makes it rather easy for the topic model to identify the topics relating to this.
The vocabulary is quite unique.
There is some overlap of course, the manual identification of topics did get a worse precision than the logistic regression model.
That is, it got more invalid reports when trying to identify the valid ones.
A probable reason for this is that there exists some information in the topics other than topic 1 and 17 that can help in identifying reports that have these as their most likely topic, but is valid.
One such thing could be a more finely tuned notion of prominent topics.
However, given the clear relationship between certain topics and categories, it might even be sufficient to have a well curated list of keywords to look for when classifying the reports.

%Are there anything in the results that stand out and need be
%analyzed and commented on? 

% How do the results relate to the
%material covered in the theory chapter? What does the theory
%imply about the meaning of the results? For example, what
%does it mean that a certain system got a certain numeric value
%in a usability evaluation; how good or bad is it? Is there
%something in the results that is unexpected based on the
%literature review, or is everything as one would theoretically
%expect?

\section{Method}
\label{sec:discussion-method}

In this chapter, the methods used are discussed.
This includes how the data used effects how reproducible the study is, the choice of active learning strategies,  and how the different experiments were conducted.
The sources used in the thesis is discussed as well.

\subsection{Data}

Results that are based on a public dataset are naturally easier to reproduce.
The method becomes more reliable in the sense that the same results can be expected by reproducing the concrete steps.
However, the clinical dataset from Sectra is not publicly available.
Thus any results that are derived from specific attributes of that dataset might not be exactly reproducible in a new environment.
The comparisons of the active learning algorithms are using the Reuters dataset, which is both public and a standard dataset for evaluation.
Reproducing these results might therefore be more reasonable.

\subsection{Active Learning Strategies Used}

In Section~\ref{sec:topic-categories} it became clear that there exists a pattern between the categories and the structure of the data.
Based on the knowledge that there exists a pattern, the initial goal was to find some methods that could exploit this.
Some active learning approaches using different forms of clustering, such as Dasgupta et al\@'s approach using hierarchical clustering~\cite{dasgupta2008hierarchical}, would be good contenders.
However, the method described by Dasgupta et al\@. is made for the single-label case with no obvious way of extending the technique into multi-label.
The same applies to the density based technique suggested by Attenberg et al\@.~\cite{attenberg2013class}.

Most of the active learning research seems to be focused on binary, and maybe multi-class classification.
Thus the methods described in Section~\ref{sec:active-learning} where the ones decided on.
Methods that are fully reliant on a models certainty, such as Binary Version Space Minimization (BSVM)~\cite{brinker2006active} are used.
Furthermore, methods incorporating some information about the data in the form of label cardinality is included as well.
These techniques are Maximum Loss Reduction with Maximum Confidence (MMC) and Adaptive Active Learning (AAL)~\cite{yang2009effective, li2013active}.
An attempt to take advantage of the structure of the data is done by selecting the initial samples from different clusters, as described in Section~\ref{sec:experiments-exp1-method}.

\subsection{Experiment 1}

The entire first and second experiments were based on the author's implementation of the algorithms described in Section~\ref{sec:active-learning}.
These were based on the algorithms in the papers, but may contain bugs or misinterpretations.
Exposure to public scrutiny may have been able to find any faults, and in turn make the results more reliable, since any bugs most definitely effect the results of the study.
The reliability of the results, given the current implementation, can be seen as rather reliable due to the averaging of the results through 5 iterations.

The model's performance is evaluated using standard metrics for text classification.
This makes it easy to validate that the result does in fact measure what it claims to do.

\subsection{Experiment 2}

The analysis of distributions is evaluated using some non-standard techniques.
Comparing imbalanced datasets in binary classification can easily be done with the ratio between the two classes.
This is harder to do in multi-label classification. 
Another common approach is to measure how well a model performs, with $F_1$-score for example.
This was already done in in research question 2, and does not measure the uniformity of the distribution explicitly, but rather implicitly by assuming that it will make the models perform better.
The evaluations that this report uses are instead fairly non-standard, but focuses on being intuitive in measuring how uniform the set is.
So the validity of the results here can rightly be criticized.
By using non-standard metrics, an argument can be made that it does not accurately achieve what it aims to do.
On the other hand, they are clearly defined, and make intuitive sense.
If if the most common categories make up most of the labels assigned, the dataset is probably not very well balanced.
The ratio between the smallest and the biggest category is an attempt to generalize the imbalance ratio used when measuring the binary case.
It too makes intuitive sense, a big ratio between them indicates that there is at least a couple of classes where the labeled set is imbalanced.

\subsection{Experiment 3}

During the exploration phase and the first experiment, the study of the LDA model and its topics was in some part based on the author's intuition.
For this reason, if another party would perform the same study they might identify other patterns.
For example, the 10\% threshold put on what is considered a prominent topic was purely based on intuition after exploring the dataset.
However, the patterns are later studied in a more objective way when they are visualized in the form of relationships between the topics and assigned labels.
The manual identification of topics in the first experiment is then compared to a more objective solution in the form of the logistic regression classifier.
Results derived from this classifier can therefore be seen as more reliable and reproducible.
Any subsequent study is more likely to obtain similar results this approach.

Another aspect of the experiment on invalid reports is the labeling process.
This was done manually by the author, without any medical knowledge.
Some reports may have been misidentified, but the nature of the labeling is rather trivial in this case.
The medical knowledge required to understand the result of an examination is far greater than the one needed to see if an examination was performed.
Which in most cases can be identified not despite of, but because of the lack of medical terms.
The number of reports labeled seems to be sufficient for the task, but it may have been improved a bit if more reports had been labeled.

A rather ironic part of the labeling of invalid reports is that it did not use an active learning system.
The analysis of the invalid reports was completed before the work with active learning started.
While the active learning system dealt with in this report focused on multi-label data, it could have been beneficial to use.
Evaluating the categorization of invalid reports by accuracy, precision, recall and F1-score are fairly standard.
The metrics have been used in a lot of text classification and information retrieval research~\cite{aggarwal2012surveyclass, bishop2006pattern}, and should enable comparisons of the results with other sources.

\subsection{Sources}

The sources used in the thesis are a mix of scientific articles and books.
One theme amongst the active learning sources is that some of them are quite old.
Some papers, such as the one by Tong et al.~\cite{tong2001support}, is from the early 2000 but provided a lot of the foundation that new techniques are based on.
Research relating to multi-label active learning is also relatively sparse, compared to multi-class or binary.
Newer techniques often seem to be focused on specific enhancement for using them with images.
Besides that, sources used to provide an overview of the field, such as Settles~\cite{settles2012active} and Tong et al\@.~\cite{tong2001active} are rather well cited, being cited by a couple of thousand papers each.


\section{Related Work}

Active learning has been researched in text classification with different approaches.
They can be seen as two categories: searching through the hypothesis space by using the uncertainty of a model, or by exploiting the structure of the data through clustering~\cite{dasgupta2008hierarchical}.

One of the common baselines for active learning is uncertainty sampling~\cite{lewis1994sequential}, that simply queries the label for the data point the model is most uncertain about.
In~\cite{dasgupta2008hierarchical} hierarchical clustering is used in an active learning system.
The labels are queried from clusters where there is a lot of uncertainty when it comes to the majority label.
By pruning the tree of clusters while querying for labels the goal is to obtain a pruning where each node mostly contains one label.

In \cite{nguyen2004active} they also take advantage of a clustering to select the samples to be labeled in a two-class environment.
They use that the data points closest to the centroids are the most important ones, and that most data points in one cluster have the same label.
What this approach has in common with a lot of the current research is that it is treating single-label or binary classification problems, which cannot be directly applied to a multi-label scenario.

Research in~\cite{brinker2006active} is dealing with the multi-label problem.
That is the paper that developed the \textit{binary version space minimization} strategy that is described in section~\ref{subsec:binmin}.
It simply takes the instance with the smallest margin among the binary classifiers, using the binary relevance scheme.
The MMC strategy~\cite{yang2009effective} that is described in section~\ref{subsec:mmc}, and the adaptive active learning strategy~\cite{li2013active} in section~\ref{subsec:adaptive-active-learning} are also techniques for managing the multi-label problem.
MMC tries to find the greatest reduction for the estimated loss.
While adaptive active learning combines an uncertainty measure with a measure of how the label cardinality differs.
Singh et al\@.~\cite{singh2009active} is another multi-label active learning approach that simply takes the minimum average of the margin among the classifiers for a data point.
For image classification, there has been some methods develop, for example ~\cite{li2004multilabel, qi2008two}.
In~\cite{li2004multilabel} the goal is to, after making predictions, selecting the sample with the biggest mean loss.
However experiments have shown that this is not as suitable for text classification~\cite{yang2009effective}.
In~\cite{qi2008two}, the approach is to use pairs of labels and samples to present to the annotator, and the aim is to minimize the Bayesian classification error.
Due to the fact that labeling for text classification is more time consuming than image classification, since you have to read an entire text, this approach is not suitable for text classification~\cite{yang2009effective}.

Active learning has been used to deal with the problem of imbalanced datasets before.
In a binary classification setting, Ertekin et al\@.~\cite{ertekin2007learning} used uncertainty sampling with SVMs to get a more balanced dataset.
In~\cite{attenberg2013class}, Attenberg et al\@. uses density based active learning to improve the class balance.
However, it does not attempt to apply this in a multi-label setting.

Using topic modeling for various clinical applications has also been done before.
Topic modeling has been a popular approach for this purpose since clinical data often is in the form of free text.
The resulting topics can also be interpreted by humans, which allow doctors to get more insight into the system.
Sarioglu et al\@.~\cite{sarioglu2013topic} to represent clinical reports with topic vectors in order to classify them.
Chan et al\@.~\cite{chan2013empirical} used topic models to analyze patient records and clinical reports from cancer patients.
In their paper, they found relationships between the content of the notes on the patients, with the data that was available on the patients' genetic mutations.
The interpretability of topics generated from an LDA model was studied on clinical reports by Arnold et al~\cite{arnold2016evaluating}.
They evaluated how interpretable topics were based on how many topics the model used.


\section{The Work in a Wider Context}

A discussion regarding how an active learning system affects the world around us is probably more interesting from a perspective of what it enables.
By being able to obtain a high quality set of labeled reports with less effort, people can create powerful machine learning models with less time spent on gathering data.
These systems can, and probably will, have a profound effect on the world.
Besides the improvements it may make to healthcare overall, it can also effect the jobs of those working within the field.
Replacing doctors with systems based on artificial intelligence may not happen any time soon.
One key advantage humans have, with the current state of the field, is creativity and emotional intelligence.
Both are things that are of great importance in healthcare and dealing with patients.
However, the jobs of doctors may change drastically given the aid these systems can provide them with.
They systems could, for example, help doctors in determining diagnoses.
Jobs that are of a more administrative nature are in more danger of being replaced in the near future.

Another interesting aspect is the trust in these systems.
In order for doctors to be able to trust the systems, the process by which the different models make decisions will have to be rather open.
Besides the understandability of model, the storage and treatment of the data is another area where there have to be trust in the systems.
Any computer system may be faced with breaches from an intruder, and the hospital records are often already digitized today.
However, automating the systems may make it easier to enforce certain procedures for how the data should be treated.
Something that might be of great importance going forward.